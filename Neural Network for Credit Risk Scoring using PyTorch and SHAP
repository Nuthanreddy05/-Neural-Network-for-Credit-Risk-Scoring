# Project: Neural Network for Credit Risk Scoring using PyTorch and SHAP

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import shap
import matplotlib.pyplot as plt

# Load dataset (replace with actual CSV)
data = pd.read_csv("credit_risk_data.csv")

# Example preprocessing (customize based on real data)
x = data.drop(columns=['default'])  # assuming 'default' is the label
y = data['default']

# Train-test split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Standardization
scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

# Convert to tensors
x_train_tensor = torch.tensor(x_train_scaled, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)
x_test_tensor = torch.tensor(x_test_scaled, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)

# Define PyTorch Neural Network
class CreditRiskNN(nn.Module):
    def __init__(self, input_dim):
        super(CreditRiskNN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 32)
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Linear(32, 16)
        self.relu2 = nn.ReLU()
        self.dropout = nn.Dropout(0.3)
        self.output = nn.Linear(16, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.relu1(self.fc1(x))
        x = self.dropout(self.relu2(self.fc2(x)))
        return self.sigmoid(self.output(x))

# Initialize model
model = CreditRiskNN(input_dim=x_train_tensor.shape[1])
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
epochs = 50
for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(x_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()
    if (epoch+1) % 10 == 0:
        print(f"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}")

# Evaluation
model.eval()
with torch.no_grad():
    predictions = model(x_test_tensor)
    preds = (predictions > 0.5).float()
    accuracy = (preds == y_test_tensor).float().mean()
    print(f"Test Accuracy: {accuracy.item() * 100:.2f}%")

# SHAP explainability
explainer = shap.DeepExplainer(model, x_train_tensor[:100])
shap_values = explainer.shap_values(x_test_tensor[:10])

# SHAP summary plot (convert tensor to numpy if needed)
shap.summary_plot(shap_values[0], x_test_tensor[:10].numpy(), feature_names=x.columns.tolist())

---

